import argparse
import pandas as pd
from pathlib import Path
import torch
from tqdm import tqdm
from statistics import mean, median
from vllm import LLM, SamplingParams
from vllm.inputs import TokensPrompt
# Use the optimized C++ version from the main toploc library
import sys
sys.path.insert(0, '../')
from toploc.poly import ProofPoly


def parse_args():
    parser = argparse.ArgumentParser(description="Run validation on model activations and commits.")

    parser.add_argument("--save_dir", type=str, required=True, help="Directory to save outputs and commits.")
    parser.add_argument("--decode_model_name", type=str, required=True, help="Model name used for decoding.")
    parser.add_argument("--validate_model_name", type=str, required=True, help="Model name used for validation.")
    parser.add_argument("--tp", type=int, default=1, help="Tensor parallel size.")
    parser.add_argument("--max_decode_tokens", type=int, default=512, help="Maximum number of tokens for decoding.")
    parser.add_argument("--decode_batching_size", type=int, default=32, help="Batch size for decoding.")
    parser.add_argument("--dtype", type=str, default="bfloat16", help="Data type for activations.")
    parser.add_argument("--attn", type=str, default="flash", help="Attention implementation for the model.")
    parser.add_argument("--scale_decode_mantissa", type=str, default="no", help="Scale decode mantissa.")

    return parser.parse_args()

SCALE_DECODE_MANTISSA = "no"

TMEAN = 10
TMEDIAN = 8
TEXP = 90

def check(activations: list[torch.Tensor], proof: list[str]) -> tuple[list[int], list[int], list[float], list[float]]:
    from toploc.C.csrc.utils import get_fp_parts
    topk_intersections: list[int] = []
    exp_intersections: list[int] = []
    mant_err_means: list[float] = []
    mant_err_medians: list[float] = []

    for act, b_poly in zip(activations, proof):
        flat_view = act.view(-1)
        prefill_topk_indices = flat_view.abs().topk(128).indices.tolist()
        prefill_topk_values = flat_view[prefill_topk_indices]
        
        poly = ProofPoly.from_bytes(b_poly)
        decode_topk_values = torch.tensor([poly(i) for i in prefill_topk_indices], dtype=torch.uint16).view(dtype=torch.bfloat16)
        decode_topk_indices = prefill_topk_indices

        prefill_exp, prefill_mants = get_fp_parts(prefill_topk_values)
        decode_exp, decode_mants = get_fp_parts(decode_topk_values)
        dec_i_2_topk_i = {index: i for i, index in enumerate(decode_topk_indices)}
        if SCALE_DECODE_MANTISSA == "down":
            decode_mants = [i // (2 ** 16) for i in decode_mants]
        elif SCALE_DECODE_MANTISSA == "up":
            decode_mants = [i * (2 ** 16) for i in decode_mants]

        topk_intersection = 0
        exp_intersection = 0
        mant_errs: list[float] = []

        for i, index in enumerate(prefill_topk_indices):
            if index in dec_i_2_topk_i:
                topk_intersection += 1
                if decode_exp[dec_i_2_topk_i[index]] == prefill_exp[i]:
                    exp_intersection += 1
                    mant_errs.append(abs(decode_mants[dec_i_2_topk_i[index]] - prefill_mants[i]))
        topk_intersections.append(topk_intersection)
        exp_intersections.append(exp_intersection)
        if len(mant_errs) == 0:
            mant_err_means.append(128.0)
            mant_err_medians.append(128.0)
        else:
            mant_err_means.append(mean(mant_errs))
            mant_err_medians.append(median(mant_errs))
      
    for mant_err_mean, mant_err_median, exp_intersection in zip(mant_err_means, mant_err_medians, exp_intersections):
        if mant_err_mean > TMEAN or mant_err_median > TMEDIAN or exp_intersection < TEXP:   
            print(f"VERIFICATION FAILED: Mantissa error mean: {mant_err_mean} above {TMEAN} or median: {mant_err_median} above {TMEDIAN} or exp intersections: {exp_intersection} below {TEXP}")
        else:
            print(f"VERIFICATION PASSED: Mantissa error mean: {mant_err_means} below {TMEAN} and median: {mant_err_medians} below {TMEDIAN} and exp intersections: {exp_intersections} above {TEXP}")
        
    return topk_intersections, exp_intersections, mant_err_means, mant_err_medians

def segment_prefill_activations(activations: torch.Tensor, max_decode_tokens: int, decode_batching_size: int) -> list[torch.Tensor]:
    ret: list[torch.Tensor] = [activations[:, :-max_decode_tokens]]
    for i in range(activations.size(1) - max_decode_tokens, activations.size(1), decode_batching_size):
        ret.append(activations[:, i:i+decode_batching_size])
    return ret

def main(args):
    global SCALE_DECODE_MANTISSA
    SCALE_DECODE_MANTISSA = args.scale_decode_mantissa
    if args.attn != "flash":
        raise NotImplementedError("Only flash attention is supported for now.")
    save_dir = Path(args.save_dir)
    outputs_path = save_dir / f'outputs_{args.decode_model_name.replace("/", "--")}.pt'
    outputs = torch.load(outputs_path)
    
    with open(save_dir / f'poly_{args.decode_model_name.replace("/", "--")}_128.bin', 'rb') as f:
        polys = [[f.read(258) for _j in range(1 + args.max_decode_tokens // args.decode_batching_size)] for _ in range(len(outputs))]
    

    # Configure for macOS compatibility
    import os
    os.environ["VLLM_CPU_KVCACHE_SPACE"] = "4"  # Use CPU for key-value cache
    
    llm = LLM(
        model=args.validate_model_name,
        tensor_parallel_size=args.tp,
        max_model_len=2048,  # Reduced for macOS
        dtype=args.dtype,
        device="cpu",  # Explicitly use CPU on macOS
        trust_remote_code=True,
    )
    model = llm.llm_engine.model_executor.driver_worker.model_runner.model

    saved_activations = []
    def activation_saving_hook(module, input, output):
        saved_activations.append(output[0].detach().clone().cpu())
    saved_activations_handle = model.model.norm.register_forward_hook(activation_saving_hook)

    names = []
    topk_intersections = []
    exp_intersections = []
    mant_err_means = []
    mant_err_medians = []

    for i, input_ids in tqdm(enumerate(outputs), total=len(outputs)):
        tokens_prompt = TokensPrompt(prompt_token_ids=input_ids[0][:-1].tolist())
        _ = llm.generate(tokens_prompt, SamplingParams(temperature=0.8, top_p=0.95, ignore_eos=True, max_tokens=1))
        activations = segment_prefill_activations(
            saved_activations[0].unsqueeze(0), args.max_decode_tokens, args.decode_batching_size
        )
        
        topk_res, exp_res, mant_means, mant_medians = check(activations, polys[i])
        # print(f"Topk: {topk_res}, Exp: {exp_res}, Mant Mean: {mant_means}, Mant Median: {mant_medians}")

        names.extend([f"Q{i}_{j}" for j in range(len(topk_res))])
        topk_intersections.extend(topk_res)
        exp_intersections.extend(exp_res)
        mant_err_means.extend(mant_means)
        mant_err_medians.extend(mant_medians)
        saved_activations = []

    df = pd.DataFrame({
        'Name': names,
        'Topk Intersections': topk_intersections,
        'Exp Intersections': exp_intersections,
        'Mant Err Means': mant_err_means,
        'Mant Err Medians': mant_err_medians
    })

    output_file = save_dir / f'poly_validation_{args.validate_model_name.replace("/", "--")}_{args.attn}_{args.dtype}_{args.tp}A100_on_{args.decode_model_name.replace("/", "--")}.parquet'
    df.to_parquet(output_file, index=False)
    print(df)

    del llm

if __name__ == "__main__":
    args = parse_args()
    main(args)
