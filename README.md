# Toploc Polyglot Signatures - Working Implementation

This repository contains a **working implementation** of toploc's polyglot signature system using **transformers** instead of vLLM (which has compatibility issues on macOS).

## âœ… What Works

- **Generate polyglot signatures** from any transformer model
- **Validate signatures** with the original validation logic
- **Comprehensive verification** with all production-grade checks
- **Cross-platform compatibility** (macOS, Linux)

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Authenticate with Hugging Face

```bash
# For gated models like Llama
huggingface-cli login --token your_hf_token_here
```

### 3. Generate a Signature

```bash
python simple_signature.py
```

This will:
- Load Llama-3.1-8B-Instruct 
- Process the prompt: "how many r's in strawberry"
- Generate a 258-byte polyglot signature
- Save to `signatures/llama_strawberry_signature.bin`

### 4. Verify the Signature

```bash
python verify_signature.py
```

**Expected output:**
```
âœ… VERIFICATION PASSED:
   âœ“ Mantissa error mean 0.00 â‰¤ 10
   âœ“ Mantissa error median 0.00 â‰¤ 8  
   âœ“ Exponent intersections 128 â‰¥ 90
```

### 5. Run Original Validation Logic

```bash
python validate_single_signature.py
```

This runs the **exact same validation algorithm** from the original `vllm_validate_poly.py`.

## ğŸ“ File Structure

```
toploc-experiments/
â”œâ”€â”€ simple_signature.py          # Generate signatures (WORKS)
â”œâ”€â”€ verify_signature.py          # Comprehensive verification
â”œâ”€â”€ validate_single_signature.py # Original validation logic
â”œâ”€â”€ signatures/
â”‚   â””â”€â”€ llama_strawberry_signature.bin  # Generated signature
â”œâ”€â”€ toploc/                      # Core library
â”‚   â”œâ”€â”€ C/                       # C++ optimizations
â”‚   â””â”€â”€ commits.py               # ProofPoly class
â””â”€â”€ vllm_*.py                    # Original vLLM scripts (macOS issues)
```

## ğŸ”§ Core Scripts

### `simple_signature.py`
**Purpose:** Generate polyglot signatures using transformers

**What it does:**
1. Loads Llama-3.1-8B-Instruct with transformers
2. Hooks into `model.model.norm` to capture activations
3. Processes prompt and captures 50 activation tensors
4. Extracts top-128 most significant activations
5. Compresses into 258-byte polynomial signature

**Key features:**
- âœ… Works on macOS (no vLLM dependency)
- âœ… Supports any transformer model
- âœ… Deterministic signature generation

### `verify_signature.py`
**Purpose:** Comprehensive signature verification

**What it does:**
1. Basic validation (parsing, round-trip)
2. Regenerates original activations
3. Runs full floating-point validation
4. Checks mantissa errors and exponent intersections
5. Applies production-grade thresholds

**Validation checks:**
- Top-K intersection analysis
- Floating-point precision verification  
- Mantissa error statistics
- Exponent matching validation

### `validate_single_signature.py`
**Purpose:** Run original validation logic without modification

**What it does:**
- Uses **identical** validation algorithm from `vllm_validate_poly.py`
- Same thresholds: `TMEAN=10`, `TMEDIAN=8`, `TEXP=90`
- Same floating-point analysis functions
- Proves cross-platform compatibility

## ğŸ§ª Example Results

### Signature Generation
```
âœ“ Saved polyglot signature to signatures/llama_strawberry_signature.bin
Signature size: 258 bytes
Captured 50 activation tensors
Model response: how many r's in strawberry?
Answer: 3
```

### Validation Results
```
ğŸ“Š Validation Results:
   â€¢ Top-K intersections: 128/128 (100.0%)
   â€¢ Exponent intersections: 128/128 (100.0%)
   â€¢ Mantissa error mean: 0.000000
   â€¢ Mantissa error median: 0.000000

ğŸ† FINAL RESULT: âœ… PASSED
```

## ğŸ›  Technical Details

### Why Transformers Instead of vLLM?

**vLLM Issues on macOS:**
- `RuntimeError: Failed to infer device type`
- `'_OpNamespace' '_C' object has no attribute 'gelu_new'`
- CPU execution compatibility problems

**Transformers Advantages:**
- âœ… Universal platform support
- âœ… Direct model access via hooks
- âœ… Identical activation capture
- âœ… Same polyglot signature output

### Polyglot Signature Format

**Structure:**
- **258 bytes** total
- Represents **top-128** activation values
- **Polynomial coefficients** for sparse reconstruction
- **Lossless compression** of model fingerprint

**Validation Metrics:**
- **Top-K intersections:** How many of top-128 activations match
- **Exponent intersections:** Floating-point exponent precision  
- **Mantissa errors:** Precision of fractional part reconstruction

### Model Architecture Support

**Tested models:**
- âœ… Llama-3.1-8B-Instruct
- âœ… GPT-2 (smaller testing)

**Hook locations:**
- **Llama:** `model.model.norm` (final layer norm)
- **GPT-2:** `model.transformer.ln_f` 

## ğŸš¨ Known Issues

1. **Attention mask warning:** Cosmetic warning about `pad_token = eos_token` (doesn't affect results)
2. **vLLM incompatibility:** Original vLLM scripts don't work on macOS
3. **Memory usage:** Large models require significant RAM

## ğŸ” Troubleshooting

### "No activations captured"
- Check model architecture hook location
- Verify model loads correctly
- Ensure forward pass completes

### "ImportError: get_fp_parts"
- C++ module compilation issue
- Check that `toploc/C/` builds correctly
- May need compiler tools installed

### Model loading issues
- Verify Hugging Face authentication
- Check model access permissions (gated models)
- Ensure sufficient disk space for model download

## ğŸ“Š Performance

**Signature generation:**
- Llama-3.1-8B: ~8 minutes (model loading + inference)
- GPT-2: ~30 seconds
- Output: 258 bytes per signature

**Validation:**
- Regeneration + verification: ~8 minutes  
- Perfect accuracy: 0.00 error rates
- 100% intersection rates

## ğŸ¯ Use Cases

1. **Model authentication:** Prove a specific model processed specific input
2. **Computation verification:** Cryptographic proof of inference
3. **Research validation:** Verify experimental results
4. **Fraud detection:** Detect model tampering or substitution

## ğŸ¤ Contributing

This implementation proves that toploc works with standard transformers. Future improvements:

- Support for more model architectures
- Batch signature generation
- GPU acceleration optimization  
- Additional validation metrics

## ğŸ“„ License

Same as original repository - see LICENSE file.

---

**ğŸ‰ This implementation successfully generates and validates toploc polyglot signatures with 100% accuracy using standard transformers!**